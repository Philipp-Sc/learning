/*

- note: NN needs to learn to predict cp on both engine game and human game database

add avg rating point gained to evaluation display
add avg time spent on move

bishops, knights, rooks might not be equal, so split them into two. left, right
only makes sense with bishops?

- Open File x
- Half Open File x
- Isolated Pawn x
- Double pawn x
- King Side Pawn Majority x
- Queen Side Pawn Majority x
- Furthest advanced pawn x
- Least advanced pawn x
- Average pawn advancement x
- Fianchetto x

- https://en.wikipedia.org/wiki/Pawn_structure Index x
- potential for each index a distance metric. (N moves till pawn_structure i) x

- Space area. Number of safe squares available for minor pieces on the central four files on ranks 2 to 4. Safe squares one, two or three squares behind a friendly pawn are counted twice. x
(- Center influence. The number of moves that can end up in the center (excl. pawns))

- Over protection by Pieces And Pawns
- Outposts: Number of Squares that can never be attacked by a opponments pawn and that is free and protected by my own pawns
- Backward pawn: Number of squares occupied by a pawn that can never be protected by another pawn
- connected passed pawns
- connected pawns
- passed pawns


- Longest Diagonal Pawn chain (Left To Right, Right to Left)

 

- Global Pattern.
-> Bishop pin Knight Pattern nf6/Bg7, nc6/Bb7, Nf3/bg4, Nc3/bb3 (H7) Knight on the rim is dim


-> Count pawns on white square, count pawns on black square x
-> P A3, A7, H7, H3 x
-> P A4, H4, A6, H6 x

-> Pawn Fork, Knight Fork, Rook double attack,.. 

- King safety (see wikipedia - pawns before king, etc)  

- Attack
Pinned direction
Knight attack
Bishop xray attack
Rook xray attack
Queen attack
Pawn attack
King attack
Attack
Queen attack diagonal
Pinned



- is middel game
- is endgame
- TYPE OF POSITION
- Tal:  Attack position/algorithm
- Capablanca: Strategical algorithm (for quiescent positions)
- Petrosian: Defense position/algorithm (the "reversed colors" Tal)

https://www.chessprogramming.org/Evaluation

- FRAGE: Sollte ich die Delta Werte/die Ver채nderung durch den letzten Zug mit an das NN 체bergeben.
- (change -> state)
- Intuitiv: Ja, aber entferne unn체tze Features, um die dimensionalit채t klein zu halten.


- Add all the features
- possibly investigate random forest
- Do PCR Analysis on the feature, take explain 90%
- (cleans the feature vector)


- Make a regression analysis with all stats and the evaluation. To find out the most relevant features.


*/
 
    // load  engine database. for every game transform it to a list of FENs
    // also transform pgn to a list of moves that show which piece is captured by which
    // count P or p
    // count B or b
    // count N or n
    // count Q or q
    // count R or r
    // transform FEN to boolean board free/not free square, check free lines 
    // # Nx, Bx, Rx, Qx, Kx, [a-g]x
    // perc of possible (B x n, N x b, BxB, NxN) exchanges count(has possible exchange) vs actual
    // percentage of influence
    // perc. castled by now

    // outward influence of bishops and knights (with pawns)
    // remove all other own pieces except pawns and king, count number of moves.

    //1) Material (xx)
    //2) Mobility (xx) (pawn mobility, piece mobility, queen mobility)
    //3) Expansion factor. (global,queenside,kingside)
    // expansion factor only pawns
    //4) Packing density. (x) == fire power
    //5) Integrety 

    // what are the lead ups likelyhoods to forec zugzwang
    // given all previous positions p with the error[a_prev-a_now] at @depth
    // integrety is the variance of the errors
    // low variance == high integrety

    // chaos 


    
/*<IonBadge>Elo: {playerElo}</IonBadge>*/



async function createAutoencoder(latent_features,inputSize){
    const model = tf.sequential();
  
    // To simulate PCA we use 1 hidden layer with a linear (relu) activation
    const encoder = tf.layers.dense({
      units: latent_features, 
      batchInputShape:[null,inputSize],          //We will input N samples X 4 columns
      activation: 'relu',
      kernelInitializer:"randomNormal",  //Randomize to avoid degenerate cases
      biasInitializer:"ones"});
    const decoder = tf.layers.dense({units: inputSize, activation: 'relu'});

    model.add(encoder);
    model.add(decoder); 
    await model.compile({optimizer: 'sgd', loss: 'meanSquaredError'});
    
    return  {
      'model':model,
      'encoder':encoder,
      'decoder':decoder
    }
}

async function trainAutoencoder(inputData,autoencoder) {
    const xs = tf.tensor2d(inputData);
    let h = await autoencoder.model.fit(xs, xs, {epochs: 5,batchSize:15,shuffle:true,validationSpit:0.1,
        callbacks: tfvis.show.fitCallbacks(
      { name: 'Training Autoencoder Performance' },
      ['loss', 'mse'],
      { height: 300, callbacks: ['onEpochEnd'] }
    )});
    xs.dispose();
    return h;
}

function getEuclidianDistance(arr1, arr2) {
        // calculate euclidian distance between two arrays
        let distTensor = tf.tidy(() => {
            const distance = tf.squaredDifference(arr1, arr2).sum().sqrt();
            return distance.dataSync()
        })
        return distTensor[0];
}// use this to only provide novel vectors to train.



// NOTE: all moves that have been already calculated at @depth20 with SF can be used to train the model.
// this way the model is learning on the go thanks to SF.

// get vector for the move played position
// get vectors for all alternative moves

// calculate labels for all vectors
// a) with Stockfish
// --> b) with the NN

// mean permutationScore (change per feature) vs ground truth

// train neural network on all vectors with labels (possibly skip)
// possibly add some random games into the mix

// calculate importance for all features


/* TO DO

1) Generate pawn structure list from game database, by creating pawn fen for every move, then count and create list top 100

2)

- create webworker for tf_chess, to prevent blocking the thread.

3) 

- add other features
- rename features to be more intuitive in UI
- add visualisation
*/